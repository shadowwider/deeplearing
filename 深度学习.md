# 深度学习

## 第三周

1. 激活函数

S型函数的缺点：求导之后最大值只有0.25，error向前一层传播时会被减少太多当层数太多时近input层更新很慢，产生梯度消失问题。

ReLUs激活函数： $$ f(x) = max(0,x)$$ 优点训练速度快避免梯度消失，缺点步伐过大梯度过大会让后面的梯度全部变成0，死掉，减小步长有助于改善这个缺点。

2. 分类函数（新）

softmax函数：$\sigma(k) =\frac{e^k}{\sum ^K_1e^k} $    for $ k = 1,2,3....K $  可以同时判断出N个类别不像s型函数是二元的。

3. 分类中常用的另一种代价函数cross-entropy

   $$D(y,\widehat{y}) = -\sum_i ^ky_iln\widehat{y}_i... $$ 其中y是softmax函数算出的预测值，y-是真实值

   利用熵的算法去计算两个向量之间的相似程度，**注意：因为真实值大多是（1，0，0，0，1，0）这种所以上述方程的顺序不能颠倒。**

4. TFLearn

```python
net = tflearn.input_data([None,100]) ##input层 100是输入向量的维度
##hidden层 n_units是该层的单元个数，如果多层hidden可以重复使用这句语句
net = tflearn.fully_connected(net, n_units, activation='ReLU')
##设置如何训练网络 sgd随机梯度下降  loss 表示cost方程使用代价熵函数
net = tflern.regression(net,optimizer='sgd',learning_rate=0.1,loss='catagorical_crossentropy')
##创建模型实例
model = tflearn.DNN(net)
##训练模型 batch_size表示一次随机梯度下降使用多少条数据，n_epoch表示所有数据被训练几遍
model.fit(trainX,trainY,validation_set=0.1,show_metric=True,batch_size=100,n_epoch=20)
##预测结果
model.predict(testX)

```

## 第五周   卷积神经网络CNN 

1. [pickle](https://blog.oldj.net/2010/05/26/python-pickle/)包是用来把python对象持久化到一个文件的工具
2. 词频统计字典化 `dict(zip(*np.unique(labels, return_counts=True)))` [解释](http://www.cnblogs.com/BeginMan/archive/2013/03/14/2959447.html)
3. ​


## 第七周 循环神经网络RNN

1. 普通的RNN是把上一层的信息也作为输入带入到下一层，从而形成循环。但是因为每一层是共享参数，所以类似于

   ​                                        $$ y = w^n x $$

   这种当n过大时反向传播求导会出现梯度爆炸（w>1）或者梯度消失(w<1)的情况，会让训练变得困难。

2. LSTMs(Long Short Term Memory Networks 长短期记忆网络)是普通RNN的改进，它对于记忆cell层进行复杂的处理，可以自动选择前置的信息是遗忘还是记忆，从而解决梯度爆炸等问题。基本结构如下图:

   ​

3. LSTM主要有三个门来控制记忆，分别是遗忘门、输入门和输出门。遗忘门决定C层哪些信息需要被忘记，输入们决定新输入的信息哪些东西需要被添加到记忆C中，输出门决定哪些信息可以被输出到下一层h中去。

   - 第一步，对于输入信号$h_{t-1}$ 和$x_t$进行线性变换再通过一个S型函数输出一组（0，1）的值，0表示完全遗忘，1表示完全保留

     ​                              $$f_t = \sigma(w_{ft}(h_{t-1},x_t)+b_{ft}) $$

   - 第二步,将之前的记忆$C_{t-1}$的输入与之乘积，得到遗忘之后的信息

     ​                             $$C_{t-1} = C_{t-1} * f_t$$       

   - 第三步，将输入信息做一个S型函数变换（记忆门）与输入信号做一个$tanh$（将值压缩到-1，1之间的函数）变换的值相乘，决定那些新信息被添加到记忆中去

     ​                             $$i_t =  \sigma(w_{it}(h_{t-1},x_t)+b_{it})  $$

     ​                            $$p_t = tanh(w_{ct}(h_{t-1},x_t)+b_{ct}) $$

     ​                           $$i_t = i_t * p_t$$

   - 第四步，将新的记忆加到记忆层中去形成新的记忆层$c_t$输出到下一层

     ​                           $$C_t = C_{t-1}+i_t$$

   - 第五步，将新的记忆经过tanh变换再和输入信息整合生成新的h层信息进入下一层

     ​                               $$h_t = tanh(C_t) * f_t  \sigma(w_{ot}(h_{t-1},x_t)+b_{ot}) $$

4. ​